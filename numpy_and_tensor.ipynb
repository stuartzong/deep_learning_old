{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "# N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "N, D_in, H, D_out = 3, 4, 2, 2\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67965798, -0.08477309,  0.64281521, -0.34688563],\n",
       "       [ 2.77727155, -0.09237655,  0.00802048,  0.32063501],\n",
       "       [-0.04064602,  0.38100734, -1.05766499,  0.15905582]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68076699,  1.32732059],\n",
       "       [ 1.44452066, -1.0294422 ],\n",
       "       [ 0.63106914,  0.74884408],\n",
       "       [ 0.35696426,  1.80052292]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.98407712, -2.28538278],\n",
       "       [ 1.87675168,  4.3647428 ],\n",
       "       [-0.08797999, -0.95181786]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.15469262, -1.11042737],\n",
       "       [-0.37643149,  1.21851607]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1800653 ,  1.1367831 ],\n",
       "       [ 0.53919874,  0.44426596],\n",
       "       [-1.10453301, -0.56561339]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.864611001842192\n",
      "1 2.8646110018421913\n",
      "2 2.864611001842191\n",
      "3 2.86461100184219\n",
      "4 2.864611001842189\n",
      "5 2.8646110018421886\n",
      "6 2.8646110018421878\n",
      "7 2.864611001842187\n",
      "8 2.8646110018421864\n",
      "9 2.864611001842186\n",
      "10 2.864611001842185\n",
      "11 2.8646110018421846\n",
      "12 2.864611001842184\n",
      "13 2.8646110018421833\n",
      "14 2.864611001842183\n",
      "15 2.8646110018421824\n",
      "16 2.8646110018421815\n",
      "17 2.8646110018421815\n",
      "18 2.8646110018421806\n",
      "19 2.86461100184218\n",
      "20 2.8646110018421798\n",
      "21 2.8646110018421793\n",
      "22 2.864611001842179\n",
      "23 2.864611001842179\n",
      "24 2.864611001842178\n",
      "25 2.8646110018421775\n",
      "26 2.864611001842177\n",
      "27 2.864611001842177\n",
      "28 2.8646110018421767\n",
      "29 2.864611001842176\n",
      "30 2.864611001842176\n",
      "31 2.8646110018421753\n",
      "32 2.8646110018421753\n",
      "33 2.864611001842175\n",
      "34 2.8646110018421744\n",
      "35 2.8646110018421744\n",
      "36 2.864611001842174\n",
      "37 2.864611001842174\n",
      "38 2.8646110018421735\n",
      "39 2.8646110018421735\n",
      "40 2.8646110018421727\n",
      "41 2.8646110018421727\n",
      "42 2.8646110018421727\n",
      "43 2.864611001842172\n",
      "44 2.8646110018421718\n",
      "45 2.8646110018421718\n",
      "46 2.8646110018421718\n",
      "47 2.8646110018421713\n",
      "48 2.8646110018421713\n",
      "49 2.864611001842171\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(50):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "   \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "   \n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: Tensors\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Like numpy arrays, PyTorch Tensors do not know anything about deep learning or computational graphs or gradients; they are a generic tool for scientific computing.\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we need to manually implement the forward and backward passes through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25233694.0\n",
      "1 23002228.0\n",
      "2 24326592.0\n",
      "3 25894804.0\n",
      "4 25161994.0\n",
      "5 20851028.0\n",
      "6 14577095.0\n",
      "7 8811716.0\n",
      "8 4958921.0\n",
      "9 2804366.5\n",
      "10 1697150.875\n",
      "11 1128627.625\n",
      "12 821972.625\n",
      "13 641584.4375\n",
      "14 524307.875\n",
      "15 440944.8125\n",
      "16 377357.71875\n",
      "17 326647.125\n",
      "18 284973.75\n",
      "19 250090.28125\n",
      "20 220447.3125\n",
      "21 195002.0\n",
      "22 173017.59375\n",
      "23 153939.390625\n",
      "24 137316.890625\n",
      "25 122774.7265625\n",
      "26 110019.1640625\n",
      "27 98791.625\n",
      "28 88886.2734375\n",
      "29 80116.6640625\n",
      "30 72346.3125\n",
      "31 65432.6953125\n",
      "32 59270.57421875\n",
      "33 53766.96484375\n",
      "34 48843.35546875\n",
      "35 44433.08203125\n",
      "36 40472.25\n",
      "37 36908.5859375\n",
      "38 33697.37890625\n",
      "39 30800.328125\n",
      "40 28183.82421875\n",
      "41 25816.892578125\n",
      "42 23672.603515625\n",
      "43 21726.181640625\n",
      "44 19961.556640625\n",
      "45 18356.17578125\n",
      "46 16894.205078125\n",
      "47 15566.021484375\n",
      "48 14355.1728515625\n",
      "49 13248.474609375\n",
      "50 12235.6767578125\n",
      "51 11308.1669921875\n",
      "52 10457.587890625\n",
      "53 9677.5361328125\n",
      "54 8962.3203125\n",
      "55 8305.2109375\n",
      "56 7700.4228515625\n",
      "57 7143.56640625\n",
      "58 6630.4501953125\n",
      "59 6157.33447265625\n",
      "60 5721.0810546875\n",
      "61 5318.3828125\n",
      "62 4946.322265625\n",
      "63 4602.12255859375\n",
      "64 4283.8037109375\n",
      "65 3989.075927734375\n",
      "66 3716.056884765625\n",
      "67 3463.35986328125\n",
      "68 3229.255615234375\n",
      "69 3011.9912109375\n",
      "70 2810.32763671875\n",
      "71 2623.026611328125\n",
      "72 2449.046630859375\n",
      "73 2287.398193359375\n",
      "74 2137.0830078125\n",
      "75 1997.3006591796875\n",
      "76 1867.1839599609375\n",
      "77 1746.0601806640625\n",
      "78 1633.2679443359375\n",
      "79 1528.2529296875\n",
      "80 1430.327880859375\n",
      "81 1339.057861328125\n",
      "82 1253.90576171875\n",
      "83 1174.4713134765625\n",
      "84 1100.3465576171875\n",
      "85 1031.193603515625\n",
      "86 966.5523681640625\n",
      "87 906.207275390625\n",
      "88 849.8228759765625\n",
      "89 797.1514892578125\n",
      "90 747.9054565429688\n",
      "91 701.8431396484375\n",
      "92 658.7535400390625\n",
      "93 618.4530029296875\n",
      "94 580.7244873046875\n",
      "95 545.4217529296875\n",
      "96 512.3687744140625\n",
      "97 481.4201354980469\n",
      "98 452.41693115234375\n",
      "99 425.2474670410156\n",
      "100 399.7952880859375\n",
      "101 375.9366455078125\n",
      "102 353.5505065917969\n",
      "103 332.5703430175781\n",
      "104 312.8944091796875\n",
      "105 294.4307556152344\n",
      "106 277.1097717285156\n",
      "107 260.8464660644531\n",
      "108 245.58914184570312\n",
      "109 231.27401733398438\n",
      "110 217.841064453125\n",
      "111 205.23165893554688\n",
      "112 193.3755340576172\n",
      "113 182.23333740234375\n",
      "114 171.75611877441406\n",
      "115 161.90939331054688\n",
      "116 152.6493377685547\n",
      "117 143.94110107421875\n",
      "118 135.75198364257812\n",
      "119 128.04998779296875\n",
      "120 120.79931640625\n",
      "121 113.97904968261719\n",
      "122 107.5555419921875\n",
      "123 101.5105209350586\n",
      "124 95.81782531738281\n",
      "125 90.45974731445312\n",
      "126 85.4095687866211\n",
      "127 80.65190887451172\n",
      "128 76.17094421386719\n",
      "129 71.94921112060547\n",
      "130 67.9687728881836\n",
      "131 64.21802520751953\n",
      "132 60.683006286621094\n",
      "133 57.347896575927734\n",
      "134 54.201107025146484\n",
      "135 51.23596954345703\n",
      "136 48.43764114379883\n",
      "137 45.799591064453125\n",
      "138 43.311134338378906\n",
      "139 40.96100616455078\n",
      "140 38.742801666259766\n",
      "141 36.64914321899414\n",
      "142 34.672523498535156\n",
      "143 32.80632019042969\n",
      "144 31.045032501220703\n",
      "145 29.381982803344727\n",
      "146 27.810260772705078\n",
      "147 26.32503318786621\n",
      "148 24.921241760253906\n",
      "149 23.595659255981445\n",
      "150 22.34422492980957\n",
      "151 21.15997314453125\n",
      "152 20.04182243347168\n",
      "153 18.983261108398438\n",
      "154 17.983470916748047\n",
      "155 17.03778076171875\n",
      "156 16.143726348876953\n",
      "157 15.297630310058594\n",
      "158 14.4977445602417\n",
      "159 13.74100112915039\n",
      "160 13.025019645690918\n",
      "161 12.34743881225586\n",
      "162 11.706049919128418\n",
      "163 11.09952163696289\n",
      "164 10.52514934539795\n",
      "165 9.981284141540527\n",
      "166 9.466523170471191\n",
      "167 8.979141235351562\n",
      "168 8.517979621887207\n",
      "169 8.080491065979004\n",
      "170 7.666913986206055\n",
      "171 7.274721145629883\n",
      "172 6.903402805328369\n",
      "173 6.5515360832214355\n",
      "174 6.218085289001465\n",
      "175 5.901954650878906\n",
      "176 5.602814674377441\n",
      "177 5.319101333618164\n",
      "178 5.050258159637451\n",
      "179 4.79495096206665\n",
      "180 4.553286552429199\n",
      "181 4.324058532714844\n",
      "182 4.1069841384887695\n",
      "183 3.9007673263549805\n",
      "184 3.70550274848938\n",
      "185 3.5201075077056885\n",
      "186 3.3440616130828857\n",
      "187 3.177518606185913\n",
      "188 3.0191149711608887\n",
      "189 2.8690061569213867\n",
      "190 2.72670316696167\n",
      "191 2.591824769973755\n",
      "192 2.4637179374694824\n",
      "193 2.342164993286133\n",
      "194 2.226773738861084\n",
      "195 2.117236614227295\n",
      "196 2.013253688812256\n",
      "197 1.9145910739898682\n",
      "198 1.8207634687423706\n",
      "199 1.7316926717758179\n",
      "200 1.6471227407455444\n",
      "201 1.5668447017669678\n",
      "202 1.4904073476791382\n",
      "203 1.417941689491272\n",
      "204 1.3490841388702393\n",
      "205 1.2836378812789917\n",
      "206 1.2214243412017822\n",
      "207 1.1622772216796875\n",
      "208 1.1061999797821045\n",
      "209 1.0527780055999756\n",
      "210 1.0020604133605957\n",
      "211 0.9536565542221069\n",
      "212 0.9078510999679565\n",
      "213 0.8642603158950806\n",
      "214 0.8228551745414734\n",
      "215 0.7834146022796631\n",
      "216 0.7458576560020447\n",
      "217 0.7103003859519958\n",
      "218 0.676369845867157\n",
      "219 0.6441280245780945\n",
      "220 0.6134673953056335\n",
      "221 0.584274411201477\n",
      "222 0.5565056204795837\n",
      "223 0.5301113724708557\n",
      "224 0.5049445033073425\n",
      "225 0.4810905158519745\n",
      "226 0.4583553373813629\n",
      "227 0.4367450177669525\n",
      "228 0.41610202193260193\n",
      "229 0.39642900228500366\n",
      "230 0.3777601420879364\n",
      "231 0.3599971830844879\n",
      "232 0.34311482310295105\n",
      "233 0.3270062506198883\n",
      "234 0.3117236793041229\n",
      "235 0.2971431612968445\n",
      "236 0.28321874141693115\n",
      "237 0.2699417471885681\n",
      "238 0.25732457637786865\n",
      "239 0.24533647298812866\n",
      "240 0.2339365929365158\n",
      "241 0.2230709046125412\n",
      "242 0.21271507441997528\n",
      "243 0.20283061265945435\n",
      "244 0.1934291571378708\n",
      "245 0.18442866206169128\n",
      "246 0.17584651708602905\n",
      "247 0.16774412989616394\n",
      "248 0.16002170741558075\n",
      "249 0.15262727439403534\n",
      "250 0.14557357132434845\n",
      "251 0.13888105750083923\n",
      "252 0.1324874460697174\n",
      "253 0.12641242146492004\n",
      "254 0.12057751417160034\n",
      "255 0.11506626754999161\n",
      "256 0.10978361964225769\n",
      "257 0.10476229339838028\n",
      "258 0.09999958425760269\n",
      "259 0.09544175863265991\n",
      "260 0.09104650467634201\n",
      "261 0.08690517395734787\n",
      "262 0.08294107764959335\n",
      "263 0.07915901392698288\n",
      "264 0.07556931674480438\n",
      "265 0.07213547080755234\n",
      "266 0.0688474178314209\n",
      "267 0.06574379652738571\n",
      "268 0.06277024000883102\n",
      "269 0.059921227395534515\n",
      "270 0.05721735581755638\n",
      "271 0.054616767913103104\n",
      "272 0.05216594785451889\n",
      "273 0.049793753772974014\n",
      "274 0.04755501449108124\n",
      "275 0.04541928321123123\n",
      "276 0.04337506368756294\n",
      "277 0.04143255203962326\n",
      "278 0.03957366943359375\n",
      "279 0.037797242403030396\n",
      "280 0.03611727058887482\n",
      "281 0.034499067813158035\n",
      "282 0.032949432730674744\n",
      "283 0.03148287534713745\n",
      "284 0.03009769320487976\n",
      "285 0.028753558173775673\n",
      "286 0.027459651231765747\n",
      "287 0.026245540007948875\n",
      "288 0.025080619379878044\n",
      "289 0.023968834429979324\n",
      "290 0.022918768227100372\n",
      "291 0.021897926926612854\n",
      "292 0.020927736535668373\n",
      "293 0.02001306228339672\n",
      "294 0.01913704164326191\n",
      "295 0.018298117443919182\n",
      "296 0.01749543473124504\n",
      "297 0.0167293231934309\n",
      "298 0.015996122732758522\n",
      "299 0.01530538871884346\n",
      "300 0.01463455893099308\n",
      "301 0.013997027650475502\n",
      "302 0.013391251675784588\n",
      "303 0.012807739898562431\n",
      "304 0.012253094464540482\n",
      "305 0.011729402467608452\n",
      "306 0.011222985573112965\n",
      "307 0.010737408883869648\n",
      "308 0.010272646322846413\n",
      "309 0.009834224358201027\n",
      "310 0.009420261718332767\n",
      "311 0.009021534584462643\n",
      "312 0.008640875108540058\n",
      "313 0.008278414607048035\n",
      "314 0.007928967475891113\n",
      "315 0.007589813321828842\n",
      "316 0.007277394644916058\n",
      "317 0.006979352328926325\n",
      "318 0.006680475547909737\n",
      "319 0.006408508401364088\n",
      "320 0.0061471532098948956\n",
      "321 0.005885760765522718\n",
      "322 0.005645876284688711\n",
      "323 0.005414278246462345\n",
      "324 0.005198873579502106\n",
      "325 0.0049830167554318905\n",
      "326 0.004776102490723133\n",
      "327 0.00458694901317358\n",
      "328 0.0044019888155162334\n",
      "329 0.004227467346936464\n",
      "330 0.0040583498775959015\n",
      "331 0.0038990138564258814\n",
      "332 0.003741712775081396\n",
      "333 0.003597620176151395\n",
      "334 0.003452663542702794\n",
      "335 0.0033204485662281513\n",
      "336 0.0031942822970449924\n",
      "337 0.0030675448942929506\n",
      "338 0.0029520532116293907\n",
      "339 0.002839815802872181\n",
      "340 0.0027323062531650066\n",
      "341 0.0026289094239473343\n",
      "342 0.002529827645048499\n",
      "343 0.002429844345897436\n",
      "344 0.0023413756862282753\n",
      "345 0.002254046965390444\n",
      "346 0.002169035840779543\n",
      "347 0.002093857154250145\n",
      "348 0.0020137960091233253\n",
      "349 0.0019423916237428784\n",
      "350 0.001874816371127963\n",
      "351 0.0018040946451947093\n",
      "352 0.0017410536529496312\n",
      "353 0.0016806178027763963\n",
      "354 0.001619944116100669\n",
      "355 0.0015657065669074655\n",
      "356 0.0015122515615075827\n",
      "357 0.0014588222838938236\n",
      "358 0.001409373595379293\n",
      "359 0.0013612195616587996\n",
      "360 0.001317171729169786\n",
      "361 0.0012728795409202576\n",
      "362 0.0012301941169425845\n",
      "363 0.0011901968391612172\n",
      "364 0.0011497028172016144\n",
      "365 0.0011118700494989753\n",
      "366 0.0010743399616330862\n",
      "367 0.0010416567092761397\n",
      "368 0.0010078981285914779\n",
      "369 0.0009773552883416414\n",
      "370 0.0009458360727876425\n",
      "371 0.0009180937777273357\n",
      "372 0.0008886039140634239\n",
      "373 0.0008625969057902694\n",
      "374 0.0008357132901437581\n",
      "375 0.0008108470938168466\n",
      "376 0.0007853018469177186\n",
      "377 0.000763916817959398\n",
      "378 0.0007406579097732902\n",
      "379 0.0007180456886999309\n",
      "380 0.0006967658991925418\n",
      "381 0.0006782125565223396\n",
      "382 0.0006588450050912797\n",
      "383 0.0006403191364370286\n",
      "384 0.0006222389056347311\n",
      "385 0.0006054185796529055\n",
      "386 0.0005871421308256686\n",
      "387 0.0005714082508347929\n",
      "388 0.000554814760107547\n",
      "389 0.000540771521627903\n",
      "390 0.0005257888115011156\n",
      "391 0.0005113977240398526\n",
      "392 0.0004977940116077662\n",
      "393 0.000484733609482646\n",
      "394 0.0004722743178717792\n",
      "395 0.00045938417315483093\n",
      "396 0.00044847538811154664\n",
      "397 0.0004374583950266242\n",
      "398 0.00042537174886092544\n",
      "399 0.0004138457588851452\n",
      "400 0.0004044843662995845\n",
      "401 0.0003939812013413757\n",
      "402 0.0003851387300528586\n",
      "403 0.00037605754914693534\n",
      "404 0.00036670055123977363\n",
      "405 0.00035736116115003824\n",
      "406 0.00034934806171804667\n",
      "407 0.0003409544879104942\n",
      "408 0.000332583615090698\n",
      "409 0.0003244411200284958\n",
      "410 0.0003165211237501353\n",
      "411 0.0003097562293987721\n",
      "412 0.00030230305856093764\n",
      "413 0.00029585399897769094\n",
      "414 0.00028893284616060555\n",
      "415 0.00028261332772672176\n",
      "416 0.00027657081955112517\n",
      "417 0.0002700455952435732\n",
      "418 0.0002635668497532606\n",
      "419 0.00025810475926846266\n",
      "420 0.0002525640884414315\n",
      "421 0.0002469440223649144\n",
      "422 0.00024197004677262157\n",
      "423 0.00023627922928426415\n",
      "424 0.0002316884056199342\n",
      "425 0.00022651035396847874\n",
      "426 0.00022206452558748424\n",
      "427 0.00021746820129919797\n",
      "428 0.00021286810806486756\n",
      "429 0.00020863697864115238\n",
      "430 0.00020481084357015789\n",
      "431 0.00020034128101542592\n",
      "432 0.0001964392577065155\n",
      "433 0.0001920387294376269\n",
      "434 0.00018862773140426725\n",
      "435 0.00018464586173649877\n",
      "436 0.00018097444262821227\n",
      "437 0.0001768868532963097\n",
      "438 0.00017386394029017538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439 0.0001704092137515545\n",
      "440 0.0001674112572800368\n",
      "441 0.00016403938934672624\n",
      "442 0.00016097180196084082\n",
      "443 0.00015809535398148\n",
      "444 0.00015529118536505848\n",
      "445 0.0001522652746643871\n",
      "446 0.0001495950564276427\n",
      "447 0.00014693527191411704\n",
      "448 0.00014424168330151588\n",
      "449 0.00014203590399120003\n",
      "450 0.0001391172845615074\n",
      "451 0.00013621925609186292\n",
      "452 0.00013383899931795895\n",
      "453 0.00013172629405744374\n",
      "454 0.00012943071487825364\n",
      "455 0.0001272310473723337\n",
      "456 0.00012451416114345193\n",
      "457 0.00012270131264813244\n",
      "458 0.00012068985233781859\n",
      "459 0.00011847742280224338\n",
      "460 0.00011672811524476856\n",
      "461 0.00011495107901282609\n",
      "462 0.00011290542170172557\n",
      "463 0.0001111301826313138\n",
      "464 0.00010928417032118887\n",
      "465 0.00010724136518547311\n",
      "466 0.000105615436041262\n",
      "467 0.00010394397395430133\n",
      "468 0.00010230938642052934\n",
      "469 0.00010035462764790282\n",
      "470 9.893770038615912e-05\n",
      "471 9.745088027557358e-05\n",
      "472 9.582490019965917e-05\n",
      "473 9.423097799299285e-05\n",
      "474 9.268492431147024e-05\n",
      "475 9.129592217504978e-05\n",
      "476 9.015011164592579e-05\n",
      "477 8.859834633767605e-05\n",
      "478 8.732100832276046e-05\n",
      "479 8.608812640886754e-05\n",
      "480 8.465237624477595e-05\n",
      "481 8.345844980794936e-05\n",
      "482 8.245922072092071e-05\n",
      "483 8.148682536557317e-05\n",
      "484 8.029294258449227e-05\n",
      "485 7.887194806244224e-05\n",
      "486 7.761290180496871e-05\n",
      "487 7.688202458666638e-05\n",
      "488 7.570545130874962e-05\n",
      "489 7.448466203641146e-05\n",
      "490 7.35028661438264e-05\n",
      "491 7.251135684782639e-05\n",
      "492 7.145808194763958e-05\n",
      "493 7.052216096781194e-05\n",
      "494 6.959035090403631e-05\n",
      "495 6.875375402159989e-05\n",
      "496 6.767392915207893e-05\n",
      "497 6.658255006186664e-05\n",
      "498 6.567627133335918e-05\n",
      "499 6.496199785033241e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# dtype = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd\n",
    "\n",
    "PyTorch: Tensors and autograd\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29700598.0\n",
      "1 25226410.0\n",
      "2 25419906.0\n",
      "3 26285604.0\n",
      "4 25099588.0\n",
      "5 20640374.0\n",
      "6 14429671.0\n",
      "7 8793727.0\n",
      "8 5029030.5\n",
      "9 2896818.25\n",
      "10 1785171.125\n",
      "11 1203988.875\n",
      "12 884656.0625\n",
      "13 693464.5625\n",
      "14 567827.0625\n",
      "15 477857.25\n",
      "16 409187.0625\n",
      "17 354365.4375\n",
      "18 309337.3125\n",
      "19 271738.6875\n",
      "20 239867.359375\n",
      "21 212605.765625\n",
      "22 189129.96875\n",
      "23 168799.96875\n",
      "24 151113.515625\n",
      "25 135676.953125\n",
      "26 122147.3359375\n",
      "27 110237.265625\n",
      "28 99716.90625\n",
      "29 90397.0859375\n",
      "30 82117.609375\n",
      "31 74735.6875\n",
      "32 68137.7734375\n",
      "33 62233.5234375\n",
      "34 56935.30078125\n",
      "35 52166.49609375\n",
      "36 47865.4296875\n",
      "37 43978.921875\n",
      "38 40459.85546875\n",
      "39 37268.65234375\n",
      "40 34368.515625\n",
      "41 31729.1171875\n",
      "42 29323.60546875\n",
      "43 27127.58984375\n",
      "44 25120.4921875\n",
      "45 23283.93359375\n",
      "46 21603.189453125\n",
      "47 20060.74609375\n",
      "48 18643.03125\n",
      "49 17338.4453125\n",
      "50 16137.263671875\n",
      "51 15030.3505859375\n",
      "52 14008.44140625\n",
      "53 13064.9765625\n",
      "54 12195.1318359375\n",
      "55 11389.380859375\n",
      "56 10643.30078125\n",
      "57 9951.5830078125\n",
      "58 9310.341796875\n",
      "59 8715.5927734375\n",
      "60 8163.0537109375\n",
      "61 7649.32568359375\n",
      "62 7172.48095703125\n",
      "63 6729.02197265625\n",
      "64 6315.8193359375\n",
      "65 5930.76513671875\n",
      "66 5571.28564453125\n",
      "67 5235.68701171875\n",
      "68 4922.287109375\n",
      "69 4629.39892578125\n",
      "70 4355.5810546875\n",
      "71 4099.3662109375\n",
      "72 3859.585205078125\n",
      "73 3635.033447265625\n",
      "74 3424.634033203125\n",
      "75 3227.4580078125\n",
      "76 3042.55810546875\n",
      "77 2869.10595703125\n",
      "78 2706.368896484375\n",
      "79 2553.552978515625\n",
      "80 2410.023681640625\n",
      "81 2275.158203125\n",
      "82 2148.393310546875\n",
      "83 2029.244384765625\n",
      "84 1917.141845703125\n",
      "85 1811.6761474609375\n",
      "86 1712.403564453125\n",
      "87 1618.928955078125\n",
      "88 1530.9105224609375\n",
      "89 1447.998291015625\n",
      "90 1369.8721923828125\n",
      "91 1296.216552734375\n",
      "92 1226.7752685546875\n",
      "93 1161.2901611328125\n",
      "94 1099.514404296875\n",
      "95 1041.21240234375\n",
      "96 986.174560546875\n",
      "97 934.2202758789062\n",
      "98 885.1570434570312\n",
      "99 838.8201904296875\n",
      "100 795.047119140625\n",
      "101 753.686279296875\n",
      "102 714.5833129882812\n",
      "103 677.6218872070312\n",
      "104 642.6668090820312\n",
      "105 609.611572265625\n",
      "106 578.3355102539062\n",
      "107 548.7265625\n",
      "108 520.7091064453125\n",
      "109 494.2016296386719\n",
      "110 469.10137939453125\n",
      "111 445.33563232421875\n",
      "112 422.83270263671875\n",
      "113 401.5210876464844\n",
      "114 381.3236083984375\n",
      "115 362.1913146972656\n",
      "116 344.0622253417969\n",
      "117 326.8769836425781\n",
      "118 310.58642578125\n",
      "119 295.1387634277344\n",
      "120 280.4930114746094\n",
      "121 266.60205078125\n",
      "122 253.42990112304688\n",
      "123 240.93264770507812\n",
      "124 229.07704162597656\n",
      "125 217.82363891601562\n",
      "126 207.1520538330078\n",
      "127 197.0181884765625\n",
      "128 187.3977508544922\n",
      "129 178.2616729736328\n",
      "130 169.59262084960938\n",
      "131 161.3601837158203\n",
      "132 153.54014587402344\n",
      "133 146.1098175048828\n",
      "134 139.05377197265625\n",
      "135 132.35011291503906\n",
      "136 125.98119354248047\n",
      "137 119.9274673461914\n",
      "138 114.17403411865234\n",
      "139 108.70684814453125\n",
      "140 103.50762939453125\n",
      "141 98.56722259521484\n",
      "142 93.86946868896484\n",
      "143 89.39925384521484\n",
      "144 85.1537094116211\n",
      "145 81.11299896240234\n",
      "146 77.27009582519531\n",
      "147 73.61376953125\n",
      "148 70.13677215576172\n",
      "149 66.82743835449219\n",
      "150 63.67989730834961\n",
      "151 60.68472671508789\n",
      "152 57.83397674560547\n",
      "153 55.12196350097656\n",
      "154 52.54008483886719\n",
      "155 50.08208465576172\n",
      "156 47.74189376831055\n",
      "157 45.5149040222168\n",
      "158 43.39286804199219\n",
      "159 41.37318801879883\n",
      "160 39.449283599853516\n",
      "161 37.61800003051758\n",
      "162 35.873722076416016\n",
      "163 34.21193313598633\n",
      "164 32.62998962402344\n",
      "165 31.122159957885742\n",
      "166 29.685945510864258\n",
      "167 28.317588806152344\n",
      "168 27.013263702392578\n",
      "169 25.771472930908203\n",
      "170 24.586679458618164\n",
      "171 23.45819664001465\n",
      "172 22.38334083557129\n",
      "173 21.357654571533203\n",
      "174 20.381786346435547\n",
      "175 19.450363159179688\n",
      "176 18.562761306762695\n",
      "177 17.716501235961914\n",
      "178 16.90968132019043\n",
      "179 16.140033721923828\n",
      "180 15.406554222106934\n",
      "181 14.70687198638916\n",
      "182 14.039443016052246\n",
      "183 13.403083801269531\n",
      "184 12.796899795532227\n",
      "185 12.218006134033203\n",
      "186 11.6659517288208\n",
      "187 11.139399528503418\n",
      "188 10.636770248413086\n",
      "189 10.157572746276855\n",
      "190 9.700358390808105\n",
      "191 9.264153480529785\n",
      "192 8.848001480102539\n",
      "193 8.450855255126953\n",
      "194 8.0719633102417\n",
      "195 7.710097789764404\n",
      "196 7.365279197692871\n",
      "197 7.035440444946289\n",
      "198 6.721401691436768\n",
      "199 6.421003818511963\n",
      "200 6.1346235275268555\n",
      "201 5.86098575592041\n",
      "202 5.60007381439209\n",
      "203 5.3508100509643555\n",
      "204 5.113380432128906\n",
      "205 4.886087894439697\n",
      "206 4.669189929962158\n",
      "207 4.462078094482422\n",
      "208 4.264271259307861\n",
      "209 4.075546741485596\n",
      "210 3.895261287689209\n",
      "211 3.7230770587921143\n",
      "212 3.5584001541137695\n",
      "213 3.401533842086792\n",
      "214 3.2516541481018066\n",
      "215 3.108250379562378\n",
      "216 2.9712467193603516\n",
      "217 2.840571403503418\n",
      "218 2.715731382369995\n",
      "219 2.59647798538208\n",
      "220 2.482262372970581\n",
      "221 2.3734753131866455\n",
      "222 2.269380807876587\n",
      "223 2.1700456142425537\n",
      "224 2.074922561645508\n",
      "225 1.984301209449768\n",
      "226 1.8975472450256348\n",
      "227 1.81471848487854\n",
      "228 1.7354493141174316\n",
      "229 1.6597322225570679\n",
      "230 1.5874601602554321\n",
      "231 1.5184190273284912\n",
      "232 1.4521334171295166\n",
      "233 1.3889669179916382\n",
      "234 1.3286725282669067\n",
      "235 1.270951509475708\n",
      "236 1.215855360031128\n",
      "237 1.1631909608840942\n",
      "238 1.1127097606658936\n",
      "239 1.0644681453704834\n",
      "240 1.0184272527694702\n",
      "241 0.9743833541870117\n",
      "242 0.9322511553764343\n",
      "243 0.8919908404350281\n",
      "244 0.8534521460533142\n",
      "245 0.8166986703872681\n",
      "246 0.7814693450927734\n",
      "247 0.7477459907531738\n",
      "248 0.7155131697654724\n",
      "249 0.6847054958343506\n",
      "250 0.6552543044090271\n",
      "251 0.6271646618843079\n",
      "252 0.600289523601532\n",
      "253 0.5744267702102661\n",
      "254 0.549791157245636\n",
      "255 0.5261937379837036\n",
      "256 0.5037181377410889\n",
      "257 0.4820801615715027\n",
      "258 0.4614792764186859\n",
      "259 0.44178372621536255\n",
      "260 0.42294594645500183\n",
      "261 0.4048050045967102\n",
      "262 0.3875272274017334\n",
      "263 0.3709670603275299\n",
      "264 0.35515573620796204\n",
      "265 0.3400123715400696\n",
      "266 0.3255770206451416\n",
      "267 0.3116982877254486\n",
      "268 0.2984735071659088\n",
      "269 0.28577736020088196\n",
      "270 0.2736315429210663\n",
      "271 0.2620083689689636\n",
      "272 0.2509101629257202\n",
      "273 0.2402719408273697\n",
      "274 0.23008468747138977\n",
      "275 0.2202688306570053\n",
      "276 0.21096855401992798\n",
      "277 0.20203635096549988\n",
      "278 0.1935446858406067\n",
      "279 0.1853196769952774\n",
      "280 0.17751173675060272\n",
      "281 0.16998961567878723\n",
      "282 0.16277727484703064\n",
      "283 0.15593282878398895\n",
      "284 0.14932680130004883\n",
      "285 0.14306116104125977\n",
      "286 0.1370534747838974\n",
      "287 0.13128669559955597\n",
      "288 0.12575797736644745\n",
      "289 0.1204819530248642\n",
      "290 0.11542099714279175\n",
      "291 0.11055674403905869\n",
      "292 0.10592876374721527\n",
      "293 0.1014939397573471\n",
      "294 0.09723775833845139\n",
      "295 0.09317978471517563\n",
      "296 0.08927064388990402\n",
      "297 0.0855264663696289\n",
      "298 0.08192179352045059\n",
      "299 0.07849688827991486\n",
      "300 0.07523878663778305\n",
      "301 0.07208454608917236\n",
      "302 0.06910566985607147\n",
      "303 0.06619412451982498\n",
      "304 0.06343380361795425\n",
      "305 0.060785070061683655\n",
      "306 0.05827290937304497\n",
      "307 0.0558445081114769\n",
      "308 0.05351290479302406\n",
      "309 0.051300518214702606\n",
      "310 0.04917457327246666\n",
      "311 0.04713873565196991\n",
      "312 0.04518526419997215\n",
      "313 0.04329521581530571\n",
      "314 0.04151095449924469\n",
      "315 0.03979552909731865\n",
      "316 0.03815602883696556\n",
      "317 0.036582089960575104\n",
      "318 0.03508079797029495\n",
      "319 0.03360984846949577\n",
      "320 0.032228000462055206\n",
      "321 0.03090754896402359\n",
      "322 0.02963179349899292\n",
      "323 0.02841711975634098\n",
      "324 0.02724900096654892\n",
      "325 0.026132619008421898\n",
      "326 0.025054723024368286\n",
      "327 0.02402029186487198\n",
      "328 0.023041676729917526\n",
      "329 0.02210407517850399\n",
      "330 0.02120538055896759\n",
      "331 0.020341020077466965\n",
      "332 0.019505327567458153\n",
      "333 0.018708962947130203\n",
      "334 0.017948484048247337\n",
      "335 0.01723102480173111\n",
      "336 0.016536753624677658\n",
      "337 0.015873871743679047\n",
      "338 0.015224240720272064\n",
      "339 0.014610969461500645\n",
      "340 0.014019966125488281\n",
      "341 0.013461263850331306\n",
      "342 0.012912836857140064\n",
      "343 0.012402896769344807\n",
      "344 0.011901949532330036\n",
      "345 0.011428860947489738\n",
      "346 0.01097229402512312\n",
      "347 0.01053602434694767\n",
      "348 0.010119730606675148\n",
      "349 0.009713510051369667\n",
      "350 0.009323089383542538\n",
      "351 0.00895311962813139\n",
      "352 0.00860640313476324\n",
      "353 0.008266648277640343\n",
      "354 0.00794285535812378\n",
      "355 0.007627035025507212\n",
      "356 0.00733175128698349\n",
      "357 0.0070456634275615215\n",
      "358 0.006771707907319069\n",
      "359 0.006508893333375454\n",
      "360 0.006258078385144472\n",
      "361 0.006020774133503437\n",
      "362 0.005794289521872997\n",
      "363 0.005567752756178379\n",
      "364 0.005352450534701347\n",
      "365 0.005149756092578173\n",
      "366 0.004956371616572142\n",
      "367 0.0047690048813819885\n",
      "368 0.004587510600686073\n",
      "369 0.004416720476001501\n",
      "370 0.004248542711138725\n",
      "371 0.004093495663255453\n",
      "372 0.003942947834730148\n",
      "373 0.003797618206590414\n",
      "374 0.003657817142084241\n",
      "375 0.003524554893374443\n",
      "376 0.0033923278097063303\n",
      "377 0.003270271932706237\n",
      "378 0.0031510291155427694\n",
      "379 0.0030383432749658823\n",
      "380 0.002929255599156022\n",
      "381 0.0028252943884581327\n",
      "382 0.002723266603425145\n",
      "383 0.0026274952106177807\n",
      "384 0.002536141313612461\n",
      "385 0.002445294987410307\n",
      "386 0.0023593325167894363\n",
      "387 0.002278681145980954\n",
      "388 0.0022002665791660547\n",
      "389 0.002124896040186286\n",
      "390 0.002050774870440364\n",
      "391 0.0019831573590636253\n",
      "392 0.0019143132958561182\n",
      "393 0.001851096167229116\n",
      "394 0.0017896494828164577\n",
      "395 0.001728985458612442\n",
      "396 0.0016723287990316749\n",
      "397 0.0016144004184752703\n",
      "398 0.001563281286507845\n",
      "399 0.0015117048751562834\n",
      "400 0.001461965381167829\n",
      "401 0.0014159969286993146\n",
      "402 0.0013717494439333677\n",
      "403 0.0013278968399390578\n",
      "404 0.0012849466875195503\n",
      "405 0.0012453198432922363\n",
      "406 0.0012053888058289886\n",
      "407 0.0011689409147948027\n",
      "408 0.0011345883831381798\n",
      "409 0.001098687294870615\n",
      "410 0.0010653440840542316\n",
      "411 0.0010328107746317983\n",
      "412 0.0010019883047789335\n",
      "413 0.0009728861041367054\n",
      "414 0.0009428096818737686\n",
      "415 0.0009161251364275813\n",
      "416 0.000889624236151576\n",
      "417 0.0008628932991996408\n",
      "418 0.0008385733235627413\n",
      "419 0.0008146214531734586\n",
      "420 0.0007916442700661719\n",
      "421 0.0007680889102630317\n",
      "422 0.0007467799005098641\n",
      "423 0.0007255824748426676\n",
      "424 0.0007045583915896714\n",
      "425 0.0006862477166578174\n",
      "426 0.0006668381975032389\n",
      "427 0.0006484744953922927\n",
      "428 0.0006311492179520428\n",
      "429 0.0006140764453448355\n",
      "430 0.0005973472143523395\n",
      "431 0.0005801737424917519\n",
      "432 0.0005644445191137493\n",
      "433 0.0005494560464285314\n",
      "434 0.0005348212434910238\n",
      "435 0.0005212269607000053\n",
      "436 0.0005080514820292592\n",
      "437 0.0004952994640916586\n",
      "438 0.0004809691454283893\n",
      "439 0.0004694286617450416\n",
      "440 0.0004576454230118543\n",
      "441 0.00044685695320367813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 0.0004352903342805803\n",
      "443 0.000425031321356073\n",
      "444 0.00041443013469688594\n",
      "445 0.0004048759874422103\n",
      "446 0.0003944701747968793\n",
      "447 0.00038468322600238025\n",
      "448 0.00037608068669214845\n",
      "449 0.0003668770077638328\n",
      "450 0.00035825828672386706\n",
      "451 0.0003499225713312626\n",
      "452 0.00034090003464370966\n",
      "453 0.0003332629567012191\n",
      "454 0.00032575841760262847\n",
      "455 0.00031867812504060566\n",
      "456 0.0003112089179921895\n",
      "457 0.00030399911338463426\n",
      "458 0.0002974079689010978\n",
      "459 0.0002906351292040199\n",
      "460 0.0002837925567291677\n",
      "461 0.00027763034449890256\n",
      "462 0.0002720996562857181\n",
      "463 0.000266393821220845\n",
      "464 0.0002599817526061088\n",
      "465 0.00025407355860807\n",
      "466 0.00024855678202584386\n",
      "467 0.00024312268942594528\n",
      "468 0.00023802924260962754\n",
      "469 0.0002326656976947561\n",
      "470 0.00022822350729256868\n",
      "471 0.00022383870964404196\n",
      "472 0.00021890435891691595\n",
      "473 0.00021444754383992404\n",
      "474 0.0002105297171510756\n",
      "475 0.00020610821957234293\n",
      "476 0.00020229077199473977\n",
      "477 0.000197950066649355\n",
      "478 0.00019415195856709033\n",
      "479 0.0001906405814224854\n",
      "480 0.00018664399976842105\n",
      "481 0.00018243357772007585\n",
      "482 0.0001795311109162867\n",
      "483 0.00017600851424504071\n",
      "484 0.0001723599125398323\n",
      "485 0.00016879763279575855\n",
      "486 0.00016512097499798983\n",
      "487 0.00016179075464606285\n",
      "488 0.00015924505714792758\n",
      "489 0.0001562129909871146\n",
      "490 0.00015374802751466632\n",
      "491 0.00015084925689734519\n",
      "492 0.00014794371963944286\n",
      "493 0.00014553367509506643\n",
      "494 0.00014265895879361778\n",
      "495 0.00014024131814949214\n",
      "496 0.00013795636186841875\n",
      "497 0.00013532995944842696\n",
      "498 0.00013315529213286936\n",
      "499 0.00013069302076473832\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# dtype = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27166500.0\n",
      "1 20970908.0\n",
      "2 18985478.0\n",
      "3 18315846.0\n",
      "4 17370156.0\n",
      "5 15410640.0\n",
      "6 12438221.0\n",
      "7 9178292.0\n",
      "8 6286936.5\n",
      "9 4127377.75\n",
      "10 2672513.0\n",
      "11 1755572.375\n",
      "12 1191745.75\n",
      "13 845611.1875\n",
      "14 628256.4375\n",
      "15 487178.96875\n",
      "16 391240.25\n",
      "17 322814.34375\n",
      "18 271749.53125\n",
      "19 232152.484375\n",
      "20 200453.203125\n",
      "21 174443.296875\n",
      "22 152718.5\n",
      "23 134373.6875\n",
      "24 118769.15625\n",
      "25 105323.9921875\n",
      "26 93676.984375\n",
      "27 83535.3046875\n",
      "28 74668.0078125\n",
      "29 66886.703125\n",
      "30 60038.5703125\n",
      "31 53990.5859375\n",
      "32 48640.02734375\n",
      "33 43893.4921875\n",
      "34 39672.90625\n",
      "35 35918.98046875\n",
      "36 32566.08203125\n",
      "37 29567.0234375\n",
      "38 26886.876953125\n",
      "39 24481.765625\n",
      "40 22317.48046875\n",
      "41 20368.8359375\n",
      "42 18611.880859375\n",
      "43 17024.146484375\n",
      "44 15586.87109375\n",
      "45 14284.61328125\n",
      "46 13102.888671875\n",
      "47 12029.373046875\n",
      "48 11053.1962890625\n",
      "49 10164.1591796875\n",
      "50 9355.1064453125\n",
      "51 8616.654296875\n",
      "52 7942.0771484375\n",
      "53 7325.32861328125\n",
      "54 6760.859375\n",
      "55 6243.7666015625\n",
      "56 5769.6474609375\n",
      "57 5334.5771484375\n",
      "58 4935.037109375\n",
      "59 4567.98779296875\n",
      "60 4230.46337890625\n",
      "61 3919.701171875\n",
      "62 3633.54248046875\n",
      "63 3369.878173828125\n",
      "64 3126.84326171875\n",
      "65 2902.53515625\n",
      "66 2695.43212890625\n",
      "67 2504.1357421875\n",
      "68 2327.35107421875\n",
      "69 2163.909423828125\n",
      "70 2012.653564453125\n",
      "71 1872.646484375\n",
      "72 1743.0133056640625\n",
      "73 1622.8436279296875\n",
      "74 1511.711669921875\n",
      "75 1408.7935791015625\n",
      "76 1313.2821044921875\n",
      "77 1224.6041259765625\n",
      "78 1142.23046875\n",
      "79 1065.7022705078125\n",
      "80 994.578857421875\n",
      "81 928.431884765625\n",
      "82 866.9329223632812\n",
      "83 809.670654296875\n",
      "84 756.3798217773438\n",
      "85 706.75146484375\n",
      "86 660.5309448242188\n",
      "87 617.466064453125\n",
      "88 577.3381958007812\n",
      "89 539.9415893554688\n",
      "90 505.0635986328125\n",
      "91 472.5298767089844\n",
      "92 442.16937255859375\n",
      "93 413.84014892578125\n",
      "94 387.3997497558594\n",
      "95 362.701171875\n",
      "96 339.6413879394531\n",
      "97 318.09454345703125\n",
      "98 297.9657287597656\n",
      "99 279.1487731933594\n",
      "100 261.55731201171875\n",
      "101 245.1122283935547\n",
      "102 229.72897338867188\n",
      "103 215.34112548828125\n",
      "104 201.8809051513672\n",
      "105 189.28506469726562\n",
      "106 177.5023651123047\n",
      "107 166.46986389160156\n",
      "108 156.13917541503906\n",
      "109 146.46737670898438\n",
      "110 137.40835571289062\n",
      "111 128.9250030517578\n",
      "112 120.97950744628906\n",
      "113 113.53398895263672\n",
      "114 106.55503845214844\n",
      "115 100.01612854003906\n",
      "116 93.88815307617188\n",
      "117 88.14269256591797\n",
      "118 82.75699615478516\n",
      "119 77.70600891113281\n",
      "120 72.9686050415039\n",
      "121 68.52666473388672\n",
      "122 64.361328125\n",
      "123 60.45266342163086\n",
      "124 56.78591537475586\n",
      "125 53.34489059448242\n",
      "126 50.118072509765625\n",
      "127 47.0885124206543\n",
      "128 44.24285888671875\n",
      "129 41.57297897338867\n",
      "130 39.06882095336914\n",
      "131 36.71598434448242\n",
      "132 34.50691223144531\n",
      "133 32.433929443359375\n",
      "134 30.485769271850586\n",
      "135 28.6569881439209\n",
      "136 26.94013023376465\n",
      "137 25.326711654663086\n",
      "138 23.811399459838867\n",
      "139 22.387718200683594\n",
      "140 21.050357818603516\n",
      "141 19.79360580444336\n",
      "142 18.613801956176758\n",
      "143 17.50411605834961\n",
      "144 16.461505889892578\n",
      "145 15.481929779052734\n",
      "146 14.560734748840332\n",
      "147 13.695022583007812\n",
      "148 12.882345199584961\n",
      "149 12.117074012756348\n",
      "150 11.39847469329834\n",
      "151 10.72248649597168\n",
      "152 10.087754249572754\n",
      "153 9.4906587600708\n",
      "154 8.928604125976562\n",
      "155 8.400708198547363\n",
      "156 7.904240131378174\n",
      "157 7.4372382164001465\n",
      "158 6.997943878173828\n",
      "159 6.584916114807129\n",
      "160 6.196813583374023\n",
      "161 5.831521987915039\n",
      "162 5.4881591796875\n",
      "163 5.164647579193115\n",
      "164 4.860693454742432\n",
      "165 4.574651718139648\n",
      "166 4.305655479431152\n",
      "167 4.052505970001221\n",
      "168 3.814371347427368\n",
      "169 3.5903091430664062\n",
      "170 3.379737377166748\n",
      "171 3.181126594543457\n",
      "172 2.9946823120117188\n",
      "173 2.819063901901245\n",
      "174 2.653886079788208\n",
      "175 2.4984679222106934\n",
      "176 2.3520748615264893\n",
      "177 2.2143356800079346\n",
      "178 2.0847346782684326\n",
      "179 1.9626951217651367\n",
      "180 1.8481295108795166\n",
      "181 1.739985704421997\n",
      "182 1.638371229171753\n",
      "183 1.5427008867263794\n",
      "184 1.4525067806243896\n",
      "185 1.3677330017089844\n",
      "186 1.2879403829574585\n",
      "187 1.2127306461334229\n",
      "188 1.1420921087265015\n",
      "189 1.0753753185272217\n",
      "190 1.0126593112945557\n",
      "191 0.9537826776504517\n",
      "192 0.8981528878211975\n",
      "193 0.8457145094871521\n",
      "194 0.796514093875885\n",
      "195 0.7502769827842712\n",
      "196 0.7065497040748596\n",
      "197 0.665518045425415\n",
      "198 0.6268458962440491\n",
      "199 0.590366005897522\n",
      "200 0.5560634136199951\n",
      "201 0.5237807035446167\n",
      "202 0.4932292103767395\n",
      "203 0.4646432399749756\n",
      "204 0.4376286268234253\n",
      "205 0.4122525751590729\n",
      "206 0.38835278153419495\n",
      "207 0.3658429682254791\n",
      "208 0.34464266896247864\n",
      "209 0.32458314299583435\n",
      "210 0.3057791590690613\n",
      "211 0.2879912555217743\n",
      "212 0.2713603377342224\n",
      "213 0.2556050419807434\n",
      "214 0.2408314049243927\n",
      "215 0.22696101665496826\n",
      "216 0.21376648545265198\n",
      "217 0.20139901340007782\n",
      "218 0.1897142231464386\n",
      "219 0.17873971164226532\n",
      "220 0.1684020608663559\n",
      "221 0.1586635261774063\n",
      "222 0.1494838148355484\n",
      "223 0.14087645709514618\n",
      "224 0.1327432245016098\n",
      "225 0.1250547170639038\n",
      "226 0.11782870441675186\n",
      "227 0.11102373898029327\n",
      "228 0.1045997366309166\n",
      "229 0.09853623062372208\n",
      "230 0.09286828339099884\n",
      "231 0.08748817443847656\n",
      "232 0.08245649933815002\n",
      "233 0.07771540433168411\n",
      "234 0.07324928790330887\n",
      "235 0.0690295398235321\n",
      "236 0.06502841413021088\n",
      "237 0.06128672882914543\n",
      "238 0.05775792896747589\n",
      "239 0.05440269038081169\n",
      "240 0.05130184441804886\n",
      "241 0.048343487083911896\n",
      "242 0.04552339389920235\n",
      "243 0.042914342135190964\n",
      "244 0.04044296219944954\n",
      "245 0.03812175244092941\n",
      "246 0.035925231873989105\n",
      "247 0.03386928141117096\n",
      "248 0.03193167597055435\n",
      "249 0.03009972907602787\n",
      "250 0.02838216722011566\n",
      "251 0.026733171194791794\n",
      "252 0.025217480957508087\n",
      "253 0.02376719005405903\n",
      "254 0.022408971562981606\n",
      "255 0.021129561588168144\n",
      "256 0.019922764971852303\n",
      "257 0.018775703385472298\n",
      "258 0.017706269398331642\n",
      "259 0.016694072633981705\n",
      "260 0.015739910304546356\n",
      "261 0.014848067425191402\n",
      "262 0.014003977179527283\n",
      "263 0.013195785693824291\n",
      "264 0.012453573755919933\n",
      "265 0.011747772805392742\n",
      "266 0.011086633428931236\n",
      "267 0.010455536656081676\n",
      "268 0.009859247133135796\n",
      "269 0.009311283938586712\n",
      "270 0.008786804042756557\n",
      "271 0.008297298103570938\n",
      "272 0.007834943942725658\n",
      "273 0.007394136395305395\n",
      "274 0.006978013087064028\n",
      "275 0.00659408001229167\n",
      "276 0.006227115169167519\n",
      "277 0.00588334770873189\n",
      "278 0.005553953815251589\n",
      "279 0.005254968535155058\n",
      "280 0.004973349627107382\n",
      "281 0.004698240198194981\n",
      "282 0.004445112310349941\n",
      "283 0.004202011041343212\n",
      "284 0.003976428881287575\n",
      "285 0.003761052154004574\n",
      "286 0.0035614969674497843\n",
      "287 0.0033744543325155973\n",
      "288 0.0031936930026859045\n",
      "289 0.0030246181413531303\n",
      "290 0.0028675778303295374\n",
      "291 0.002721484750509262\n",
      "292 0.0025777986738830805\n",
      "293 0.0024471816141158342\n",
      "294 0.002321287291124463\n",
      "295 0.0022039064206182957\n",
      "296 0.0020932615734636784\n",
      "297 0.0019909723196178675\n",
      "298 0.0018908517668023705\n",
      "299 0.0017960258992388844\n",
      "300 0.00170620228163898\n",
      "301 0.0016224895371124148\n",
      "302 0.0015478983987122774\n",
      "303 0.0014746314845979214\n",
      "304 0.0014029871672391891\n",
      "305 0.0013371583772823215\n",
      "306 0.0012735100463032722\n",
      "307 0.001213128212839365\n",
      "308 0.0011601633159443736\n",
      "309 0.001107892137952149\n",
      "310 0.0010562072275206447\n",
      "311 0.0010097911581397057\n",
      "312 0.0009645954123698175\n",
      "313 0.0009234924800693989\n",
      "314 0.0008829956059344113\n",
      "315 0.000845175061840564\n",
      "316 0.0008081991691142321\n",
      "317 0.0007737429696135223\n",
      "318 0.000741431547794491\n",
      "319 0.0007130837766453624\n",
      "320 0.0006835938547737896\n",
      "321 0.0006562165799550712\n",
      "322 0.0006277404609136283\n",
      "323 0.0006032240344211459\n",
      "324 0.0005795106990262866\n",
      "325 0.0005567778134718537\n",
      "326 0.0005354974418878555\n",
      "327 0.0005146440234966576\n",
      "328 0.0004947388079017401\n",
      "329 0.0004772329411935061\n",
      "330 0.0004587489238474518\n",
      "331 0.0004412397975102067\n",
      "332 0.0004252195358276367\n",
      "333 0.00040798631380312145\n",
      "334 0.0003945569333154708\n",
      "335 0.00037961514317430556\n",
      "336 0.00036596404970623553\n",
      "337 0.0003534012066666037\n",
      "338 0.00034092363785021007\n",
      "339 0.0003293301269877702\n",
      "340 0.0003180803614668548\n",
      "341 0.0003083939664065838\n",
      "342 0.0002970075001940131\n",
      "343 0.0002877472434192896\n",
      "344 0.00027795255300588906\n",
      "345 0.00026869639987125993\n",
      "346 0.00025974243180826306\n",
      "347 0.0002514678635634482\n",
      "348 0.00024326780112460256\n",
      "349 0.00023523432901129127\n",
      "350 0.00022853389964438975\n",
      "351 0.00022198594524525106\n",
      "352 0.00021471106447279453\n",
      "353 0.00020762592612300068\n",
      "354 0.00020139111438766122\n",
      "355 0.0001958501379704103\n",
      "356 0.00019034557044506073\n",
      "357 0.00018490942602511495\n",
      "358 0.00017957408272195607\n",
      "359 0.00017386018589604646\n",
      "360 0.0001689340133452788\n",
      "361 0.00016476860037073493\n",
      "362 0.00016041647177189589\n",
      "363 0.0001555627241032198\n",
      "364 0.00015136454021558166\n",
      "365 0.00014732530689798295\n",
      "366 0.00014397013001143932\n",
      "367 0.00014041030954103917\n",
      "368 0.00013613620831165463\n",
      "369 0.00013279491395223886\n",
      "370 0.00012893311213701963\n",
      "371 0.00012565203360281885\n",
      "372 0.000122868426842615\n",
      "373 0.00012026820331811905\n",
      "374 0.00011676462600007653\n",
      "375 0.00011381592776160687\n",
      "376 0.00011147271288791671\n",
      "377 0.00010875987209146842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 0.00010606206342345104\n",
      "379 0.00010349374497309327\n",
      "380 0.00010111406299984083\n",
      "381 9.864105959422886e-05\n",
      "382 9.646104444982484e-05\n",
      "383 9.452350786887109e-05\n",
      "384 9.235912875737995e-05\n",
      "385 9.023745951708406e-05\n",
      "386 8.815152250463143e-05\n",
      "387 8.629106741864234e-05\n",
      "388 8.44119640532881e-05\n",
      "389 8.23202935862355e-05\n",
      "390 8.063963468885049e-05\n",
      "391 7.858459139242768e-05\n",
      "392 7.688406913075596e-05\n",
      "393 7.543944229837507e-05\n",
      "394 7.373918924713507e-05\n",
      "395 7.214029028546065e-05\n",
      "396 7.06145801814273e-05\n",
      "397 6.898605352034792e-05\n",
      "398 6.752143235644326e-05\n",
      "399 6.633148586843163e-05\n",
      "400 6.514985579997301e-05\n",
      "401 6.352246418828145e-05\n",
      "402 6.249133730307221e-05\n",
      "403 6.112233677413315e-05\n",
      "404 6.0162070440128446e-05\n",
      "405 5.900589167140424e-05\n",
      "406 5.779591447208077e-05\n",
      "407 5.642140968120657e-05\n",
      "408 5.5429751228075475e-05\n",
      "409 5.435301864054054e-05\n",
      "410 5.3424781071953475e-05\n",
      "411 5.248595334705897e-05\n",
      "412 5.151376171852462e-05\n",
      "413 5.076089655631222e-05\n",
      "414 4.993747643311508e-05\n",
      "415 4.9007936468115076e-05\n",
      "416 4.795468703377992e-05\n",
      "417 4.720059223473072e-05\n",
      "418 4.625223664334044e-05\n",
      "419 4.555089617497288e-05\n",
      "420 4.471564170671627e-05\n",
      "421 4.385306965559721e-05\n",
      "422 4.339047154644504e-05\n",
      "423 4.241057831677608e-05\n",
      "424 4.1785948269534856e-05\n",
      "425 4.1062252421397716e-05\n",
      "426 4.0325172449229285e-05\n",
      "427 3.981458212365396e-05\n",
      "428 3.927171928808093e-05\n",
      "429 3.861695222440176e-05\n",
      "430 3.785296212299727e-05\n",
      "431 3.713245678227395e-05\n",
      "432 3.6662848287960514e-05\n",
      "433 3.628703416325152e-05\n",
      "434 3.54986623278819e-05\n",
      "435 3.4761589631671086e-05\n",
      "436 3.4456206776667386e-05\n",
      "437 3.393474253243767e-05\n",
      "438 3.3627387892920524e-05\n",
      "439 3.307098086224869e-05\n",
      "440 3.266942803747952e-05\n",
      "441 3.2108004234032705e-05\n",
      "442 3.170696072629653e-05\n",
      "443 3.12304291583132e-05\n",
      "444 3.071470928261988e-05\n",
      "445 3.0400740797631443e-05\n",
      "446 2.9805705707985908e-05\n",
      "447 2.944493098766543e-05\n",
      "448 2.9095119316480123e-05\n",
      "449 2.853177284123376e-05\n",
      "450 2.8102947908337228e-05\n",
      "451 2.7688269256032072e-05\n",
      "452 2.741490607149899e-05\n",
      "453 2.6886316845775582e-05\n",
      "454 2.648053487064317e-05\n",
      "455 2.627554385981057e-05\n",
      "456 2.5910740077961236e-05\n",
      "457 2.5502598873572424e-05\n",
      "458 2.5237106456188485e-05\n",
      "459 2.499366928532254e-05\n",
      "460 2.463346390868537e-05\n",
      "461 2.427666913717985e-05\n",
      "462 2.3998598408070393e-05\n",
      "463 2.358026904403232e-05\n",
      "464 2.321165993635077e-05\n",
      "465 2.3027556380839087e-05\n",
      "466 2.2862670448375866e-05\n",
      "467 2.2588821593672037e-05\n",
      "468 2.2215224817045964e-05\n",
      "469 2.1978716176818125e-05\n",
      "470 2.1667014152626507e-05\n",
      "471 2.133991802111268e-05\n",
      "472 2.1193574866629206e-05\n",
      "473 2.1053287127870135e-05\n",
      "474 2.075361589959357e-05\n",
      "475 2.0512852643150836e-05\n",
      "476 2.0291465261834674e-05\n",
      "477 2.0187671907478943e-05\n",
      "478 1.986822098842822e-05\n",
      "479 1.960635927389376e-05\n",
      "480 1.9462786440271884e-05\n",
      "481 1.9204167983843945e-05\n",
      "482 1.8979611922986805e-05\n",
      "483 1.884209632407874e-05\n",
      "484 1.8561289834906347e-05\n",
      "485 1.8316313799005002e-05\n",
      "486 1.8092707250616513e-05\n",
      "487 1.7847400158643723e-05\n",
      "488 1.772952418832574e-05\n",
      "489 1.7616397599340416e-05\n",
      "490 1.7413431123713963e-05\n",
      "491 1.727870767354034e-05\n",
      "492 1.7035896235029213e-05\n",
      "493 1.686005634837784e-05\n",
      "494 1.675807470746804e-05\n",
      "495 1.665262971073389e-05\n",
      "496 1.651142338232603e-05\n",
      "497 1.6384117770940065e-05\n",
      "498 1.6198655430343933e-05\n",
      "499 1.594613968336489e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# dtype = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the backpropagation and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 3, 4, 2, 1\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 6.60897159576416\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "#     y_pred\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t ==499:\n",
    "        print(t, loss.item())\n",
    "   \n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access and gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        # what are these parameters?\n",
    "        for param in model.parameters():            \n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3065, -0.6920, -2.0258,  0.2677],\n",
       "        [-0.4559, -0.4160, -0.3827,  0.3101],\n",
       "        [-1.2335, -1.3084,  0.9000,  1.0761]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3008],\n",
       "        [-0.2863],\n",
       "        [-0.2863]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4009],\n",
       "        [-0.7626],\n",
       "        [ 1.5809]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1920, -0.2201, -0.2183,  0.0186],\n",
      "        [ 0.1046,  0.3282, -0.1726, -0.0245]])\n",
      "Parameter containing:\n",
      "tensor([-0.3493, -0.1472])\n",
      "Parameter containing:\n",
      "tensor([[-0.5682, -0.1369]])\n",
      "Parameter containing:\n",
      "tensor([-0.2857])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-90b06bc3971c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m ts1 = torch.tensor([[-0.4191,  0.7407, -0.1794, -0.2367],\n\u001b[0;32m----> 2\u001b[0;31m         [ 0.3248, -0.2662, -0.2573, -0.0398]].T)\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "ts1 = torch.tensor([[-0.4191,  0.7407, -0.1794, -0.2367],\n",
    "        [ 0.3248, -0.2662, -0.2573, -0.0398]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4191,  0.7407, -0.1794, -0.2367],\n",
       "        [ 0.3248, -0.2662, -0.2573, -0.0398]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1t = ts1.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = torch.mm(x, ts1t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7253,  1.0946],\n",
       "        [ 1.3099, -0.5605],\n",
       "        [ 0.1304,  0.3551]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5273,  0.4526])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts2 = torch.tensor(([ 0.5273,  0.4526]))#.transpose(1,0)\n",
    "ts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5273,  0.4526])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts2.transpose(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-bbac9485a3c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.mm(a1, ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what exactly does backward do?\n",
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "out = x.pow(2).sum()\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.],\n",
       "        [ 1.,  1.]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x.grad.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  2.],\n",
       "        [-2.,  2.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  0.],\n",
       "        [ 0.,  4.]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3]], dtype=torch.int8)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.ones((2,3), dtype=torch.int8)\n",
    "data = [[0, 1], [2, 3]]\n",
    "tensor.new_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[0, 1], [2, 3]]\n",
    "torch.tensor(data).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1],\n",
       "        [ 1,  1,  1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
